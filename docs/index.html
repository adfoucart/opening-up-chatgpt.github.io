<!DOCTYPE html>

<html>
<head>
<title>Opening up ChatGPT: tracking openness, transparency, and accountability in instruction-tuned text generators</title>
<link href="styles.css" rel="stylesheet"/>
<link href="favicon.png" rel="icon" type="image/x-icon"/>
<script data-domain="opening-up-chatgpt.github.io" defer="" src="https://plausible.io/js/script.js"></script>
</head>
<body>
<div id="header">
<h1><img alt="Opening up AI logo" id="title-logo" src="logos/openchatgpt-logo-favicon-red-on-transparent.png"/>Opening up ChatGPT</h1>
</div>
<div id="content">
<p>There is a growing amount of instruction-tuned text generators billing themselves as 'open source'. How open are they really?</p>
<p class="warning"><strong>Note</strong>: This is a soft launch. The paper will be available soon. Get in touch with <a href="mailto:mark.dingemanse@ru.nl">mark.dingemanse@ru.nl</a> for a sneak preview.</p>
<!--<p>Check out <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">the paper</a> or contribute to <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/">the repo</a>.</p>-->
<div id="included-table"><table>
<thead>
<tr class="main-header"><th>Project</th><th colspan="6">Availability</th><th colspan="6">Documentation</th><th colspan="2">Access</th></tr>
<tr class="second-header"><th>(maker, bases, URL)</th><th>Open code</th><th>LLM data</th><th>LLM weights</th><th>RLHF data</th><th>RLHF weights</th><th>License</th><th>Code</th><th>Architecture</th><th>Preprint</th><th>Paper</th><th>Modelcard</th><th>Datasheet</th><th>Package</th><th>API</th></tr>
</thead>
<tbody>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="">xmtf</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="Repository provides a guided overview to all components">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#data" target="_blank" title="Data made available &amp; documented in detail in repo and preprint">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#models" target="_blank" title="Model made available on github">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/bigscience/xP3all" target="_blank" title="From the documentation 'xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts &amp; datasets across 46 of languages &amp; 16 NLP tasks'">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/bigscience/bloomz-optimizer-states/tree/main" target="_blank" title="Fine-tuned checkpoint available for download">~</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf/blob/master/LICENSE.md" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="Code well documented and actively maintained">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#create-xp3x" target="_blank" title="Architecture described in preprint, code available in github repo, recipe on HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2211.01786" target="_blank" title="Preprint (updated May 2023) of 9 pages +114 page appendix">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/bigscience/bloomz" target="_blank" title="Model card">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/bigscience-workshop" target="_blank" title="bigscience-workshop">bigscience-workshop</a></td><td class="llmbase" colspan="3">LLM base: BLOOMZ, mT0</td><td class="rlbase" colspan="3">RL base: xP3</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/xmtf.yaml" target="_blank" title="xmtf.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://open-assistant.io/" target="_blank" title="">Open Assistant</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" title="Code includes guide for developers">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant/tree/main/data/datasets" target="_blank" title="Datasets documented in detail and recipes for cleaning up and downloading provided in code notebooks.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/OpenAssistant" target="_blank" title="Model weights in several variants downloadable through HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/OpenAssistant/oasst1" target="_blank" title="OpenAssistant Conversations is 'a human-generated, human-annotated assistant-style conversation corpus consisting of 161443 messages distributed across 66497 conversation trees, in 35 different languages, annotated with 461292 quality ratings' (preprint)">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF weights not separately released">✘</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/docs/faq#what-license-does-open-assistant-use" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/docs/intro" target="_blank" title="Separate website provides entry point to comprehensive documentation">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant/tree/main/model" target="_blank" title="Instructions to tune the pipeline on training data">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs//2304.07327" target="_blank" title="Preprint describes creation of OpenAssistant Conversations corpus for instruction tuning, but not the base LLM, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or published data audit found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/api" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://open-assistant.io/" target="_blank" title="LAION-AI">LAION-AI</a></td><td class="llmbase" colspan="3">LLM base: Pythia 12B</td><td class="rlbase" colspan="3">RL base: OpenAssistant Conversations</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Open-Assistant.yaml" target="_blank" title="Open-Assistant.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B" target="_blank" title="">OpenChatKit</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenDataHub" target="_blank" title="Training data curated and shared in separate repository">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B/tree/main" target="_blank" title="Model weights available via HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/laion/OIG" target="_blank" title="From the documentation 'This is our attempt to create a large instruction dataset of medium quality along with a smaller high quality instruciton dataset (OIG-small-chip2).'">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="RL weights not separately made available">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B#model-details" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenChatKit" target="_blank" title="Actively maintained repository">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenChatKit#reproducing-pythia-chat-base-7b" target="_blank" title="Architecture and recipe for reproducing model provided">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.01373" target="_blank" title="Preprint describes LM base (Pythia) but not instruction tuning details">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B" target="_blank" title="Model card partially available but fairly minimally specified">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/laion/OIG" target="_blank" title="OIG instruction dataset documented">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/togethercomputer" target="_blank" title="togethercomputer">togethercomputer</a></td><td class="llmbase" colspan="3">LLM base: EleutherAI pythia</td><td class="rlbase" colspan="3">RL base: OIG</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/OpenChatKit.yaml" target="_blank" title="OpenChatKit.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/databrickslabs/dolly" target="_blank" title="">dolly</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.01373" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.databricks.com" target="_blank" title="databricks">databricks</a></td><td class="llmbase" colspan="3">LLM base: EleutherAI pythia</td><td class="rlbase" colspan="3">RL base: databricks-dolly-15k</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/dolly.yaml" target="_blank" title="dolly.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/CarperAI/trlx" target="_blank" title="">trlx</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/CarperAI/trlx" target="_blank" title="carperai">carperai</a></td><td class="llmbase" colspan="3">LLM base: various (pythia, flan, OPT)</td><td class="rlbase" colspan="3">RL base: various</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/trlx.yaml" target="_blank" title="trlx.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="hhttps://github.com/ethanyanjiali/minChatGPT" target="_blank" title="">minChatGPT</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/ethanyanjiali/minChatGPT" target="_blank" title="ethanyanjiali">ethanyanjiali</a></td><td class="llmbase" colspan="3">LLM base: GPT2</td><td class="rlbase" colspan="3">RL base: anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/minChatGPT.yaml" target="_blank" title="minChatGPT.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-v1.3" target="_blank" title="Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.">Vicuna 13B v 1.3</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat" target="_blank" title="Actively maintained repository">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset" target="_blank" title="Vicuna is fine-tuned LLaMA, and LLaMA in turn is based on 'publicly available datasets' that are not all specified or easily downloadable.">~</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" title="Unlike Vicuna 13B v0, these weights do not require applying delta">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/lm-sys/FastChat#fine-tuning" target="_blank" title="From the documentation 'We will not release the ShareGPT dataset'. Also 'Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 140K conversations collected from ShareGPT.com.'">✘</a></td><td class="closed data-cell"><a href="https://github.com/lm-sys/FastChat#fine-tuning" target="_blank" title="No model weights are shared for the instruction tuning">✘</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" title="From the documentation 'Vicuna is based on LLaMA and should be used under LLaMA's model license.'">~</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat" target="_blank" title="Code is quite well-documented and released as part of the FastChat framework.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2306.05685.pdf" target="_blank" title="Preprint covers training of the Vicuna model.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-v1.3" target="_blank" title="Minimal model card, but many details are not provided or have to be pieced together from elsewhere.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet provided.">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/fschat/0.1.2/" target="_blank" title="Available via pip">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat#api" target="_blank" title="Support provided for several APIs OpenAI restful, HuggingFace, Langchain">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://lmsys.org/" target="_blank" title="LMSYS">LMSYS</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: ShareGPT</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/vicuna13B-lmsys.yaml" target="_blank" title="vicuna13B-lmsys.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="">Falcon-40B-instruct</a></td><td class="open data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" target="_blank" title="From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (&gt;80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct/tree/main" target="_blank" title="Model weights available through HuggingFace library">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/project-baize/baize-chatbot" target="_blank" title="RL data inherited from Baize but provenance not well-documented. From the documentation 'Falcon-40B-Instruct was finetuned on a 150M tokens from Baize mixed with 5% of RefinedWeb data.'">~</a></td><td class="open data-cell"><a href="https://github.com/project-baize/baize-chatbot#v1" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="First release came with a legally murky license that was swiftly criticised and now generates a 404. Current documentation 'Falcon-40B-Instruct is made available under the Apache 2.0 license.'">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2306.01116" target="_blank" title="Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper known.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no datasheet available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no package.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="There is no API, and HuggingFace inference API is disabled for this model.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://falconllm.tii.ae" target="_blank" title="Technology Innovation Institute">Technology Innovation Institute</a></td><td class="llmbase" colspan="3">LLM base: Falcon 40B</td><td class="rlbase" colspan="3">RL base: Baize (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Falcon-40B-instruct.yaml" target="_blank" title="Falcon-40B-instruct.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/BlinkDL/ChatRWKV" target="_blank" title="">ChatRWKV</a></td><td class="open data-cell"><a href="https://github.com/BlinkDL/ChatRWKV" target="_blank" title="Various community-contributed enhancements available">✔︎</a></td><td class="partial data-cell"><a href="https://pile.eleuther.ai/" target="_blank" title="Trained on The Pile. Recent versions also build on Red Pajama (https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)">~</a></td><td class="open data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-world/tree/main" target="_blank" title="Model weights released across different HuggingFace spaces">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Instruction tuning data not separately available. Documentation 'These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more'">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Weights not separately available.">✘</a></td><td class="open data-cell"><a href="https://github.com/BlinkDL/ChatRWKV/blob/main/LICENSE" target="_blank" title="Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code documentation scattered across github repo and HuggingFace spaces">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Architecture described in preprint (LM part) but not all details clearly documented.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2305.13048" target="_blank" title="Preprint covers only LLM (RNN based), not instruction fine-tuning, so partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or published data audit known">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" title="No modelcard, HuggingFace spaces only used to share files">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" title="No data sheet, HuggingFac spaces only used to share files">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/rwkv/" target="_blank" title="Available through pip install rwkv">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="API via HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.rwkv.com/" target="_blank" title="BlinkDL/RWKV">BlinkDL/RWKV</a></td><td class="llmbase" colspan="3">LLM base: RWKV-LM</td><td class="rlbase" colspan="3">RL base: alpaca, shareGPT (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ChatRWKV.yaml" target="_blank" title="ChatRWKV.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/Cerebras" target="_blank" title="">Cerebras-GPT-111M</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.03208" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/Cerebras" target="_blank" title="Cerebras + Schramm">Cerebras + Schramm</a></td><td class="llmbase" colspan="3">LLM base: </td><td class="rlbase" colspan="3">RL base: Alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Cerebras-GPT-111m.yaml" target="_blank" title="Cerebras-GPT-111m.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://www.mosaicml.com/blog/mpt-7b" target="_blank" title="">MPT-7B</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.mosaicml.com" target="_blank" title="MosaicML">MosaicML</a></td><td class="llmbase" colspan="3">LLM base: MosaicML</td><td class="rlbase" colspan="3">RL base: dolly, anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/MPT-7b-instruct.yaml" target="_blank" title="MPT-7b-instruct.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/LianjiaTech/BELLE" target="_blank" title="">BELLE</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="LLaMA based but copyright status unclear">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="LLaMA licence agreement">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2303.14742" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="http://www.ke.com" target="_blank" title="KE Technologies">KE Technologies</a></td><td class="llmbase" colspan="3">LLM base: LLaMA &amp; BLOOMZ</td><td class="rlbase" colspan="3">RL base: alpaca &amp; shareGPT (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/BELLE.yaml" target="_blank" title="BELLE.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" title="project_notes">Stanford Alpaca</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="LLaMA based, copyright status unclear">~</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform" target="_blank" title="LLaMA licence agreement">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://crfm.stanford.edu/" target="_blank" title="Stanford University CRFM">Stanford University CRFM</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: Self-Instruct</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/alpaca.yaml" target="_blank" title="alpaca.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/oobabooga/text-generation-webui" target="_blank" title="">text-generation-webui</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/models?sort=downloads&amp;search=eleutherai%2Fpythia+deduped" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://oobabooga.github.io" target="_blank" title="Oobabooga">Oobabooga</a></td><td class="llmbase" colspan="3">LLM base: various</td><td class="rlbase" colspan="3">RL base: various</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/text-generation-webui.yaml" target="_blank" title="text-generation-webui.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/" target="_blank" title="From the documentation 'Koala is fine-tuned on freely available interaction data scraped from the web, but with a specific focus on data that includes interaction with highly capable closed-source models such as ChatGPT.'">Koala 13B</a></td><td class="open data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Code scattered across projects and repositories">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/koala_data_pipeline" target="_blank" title="Repo contains data pipeline for preprocessing. Based on LLaMA which is said to be based on 'publicly available datasets' which are not made directly available.">~</a></td><td class="partial data-cell"><a href="https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl" target="_blank" title="Model weights only made available as a diff against LLaMA. OpenLLaMA provides a possible alternative?">~</a></td><td class="partial data-cell"><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/#datasets-and-training" target="_blank" title="Datasets described in blog post but not all made available">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RL weights not made available separately">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/young-geng/koala#license" target="_blank" title="Licensing is 'subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT'">~</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Code scattered across various repositories and not systematically documented.">~</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Architecture visually illustrated and described in some detail.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No systematic data sheet available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://bair.berkeley.edu/" target="_blank" title="BAIR">BAIR</a></td><td class="llmbase" colspan="3">LLM base: LLaMA 13B</td><td class="rlbase" colspan="3">RL base: HC3, ShareGPT, alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/koala.yaml" target="_blank" title="koala.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets">StableVicuna-13B</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main" target="_blank" title="Some elements of the code made available through HuggingFace">~</a></td><td class="open data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="All datasets are available and linked. StableVicuna-13B is fine-tuned on a mix of three datasets. OpenAssistant Conversations Dataset (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161443 messages distributed across 66497 conversation trees, in 35 different languages; GPT4All Prompt Generations, a dataset of 400k prompts and responses generated by GPT-4; and Alpaca, a dataset of 52000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta#apply-delta-weights" target="_blank" title="Model not functional out of the box as weights require a delta computation. From the docs 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="From the documentation 'The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.'">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/discussions/7" target="_blank" title="The HuggingFace community page has an open question for release of the RL model">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="CC-BY-NC-SA-4.0. License for LLaMA is more murky, hence partial. As they say 'License for the base LLaMA model's weights is Meta's non-commercial bespoke license.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main" target="_blank" title="Code is minimally documented and deployment requires non-trivial configuration, e.g. 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2302.13971" target="_blank" title="Preprint covers only the LLaMA base model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://carper.ai" target="_blank" title="CarperAI">CarperAI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: OASST1 (human), GPT4All (human), Alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/stablevicuna.yaml" target="_blank" title="stablevicuna.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://chat.openai.com/" target="_blank" title="NA">ChatGPT</a></td><td class="closed data-cell"><a href="https://chat.openai.com/" target="_blank" title="OpenAI has not released any source code related to ChatGPT">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://openai.com/" target="_blank" title="OpenAI">OpenAI</a></td><td class="llmbase" colspan="3">LLM base: GPT 3.5</td><td class="rlbase" colspan="3">RL base: Instruct-GPT</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/chatgpt.yaml" target="_blank" title="chatgpt.yaml">§</a></td></tr>
</tbody>
</table>
</div>
<h2>How to contribute</h2>
<p>If you know a model that should be listed here or a data point that needs updating, please see <a href="https://github.com/opening-up-chatgpt/">guidelines for contributors</a>. We welcome any contribution, whether it's a quick addition to our <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt">awesomelist</a> or a more detail-oriented contribution to the metadata for a <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.io/projects/">specific project</a>.</p>
<h2>Useful? Read &amp; cite the paper</h2>
<p>Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In <em>Proceedings of CUI’23</em>. July 19-21, Eindhoven. doi: 10.1145/3571884.3604316.</p>
<h2>TL;DR</h2>
<p>Our paper makes the following contributions:</p>
<ul>
<li>We review the risks of relying on proprietary software, from blackboxed processes to vendor lock-in and corporate capture of collective intelligence </li>
<li>We review best practices for open, transparent and accountable AI</li>
<li>We find over 15 ChatGPT alternatives (and counting) at varying degrees of openness, development and documentation</li>
<li>We argue that tech is never a <em>fait accompli</em>: there was never a better time for critical and constructive work in this space, and open initiatives will play a key role here</li>
</ul>
<p>We find the following recurrent patterns:</p>
<ul>
<li>Many projects inherit data of dubious legality</li>
<li>Few projects share the all-important instruction-tuning</li>
<li>Preprints are rare, peer-reviewed papers even rarer</li>
<li>Synthetic instruction-tuning data is on the rise, with unknown consequences that are in need of research</li>
</ul>
<p>We conclude as follows:</p>
<blockquote id="conclusion">Openness is not the full solution to the scientific and ethical challenges of conversational text generators. Open data will not mitigate the harmful consequences of thoughtless deployment of large language models, nor the questionable copyright implications of scraping all publicly available data from the internet. However, openness does make original research possible, including efforts to build reproducible workflows and understand the fundamentals of LLM + RLHF architectures. Openness also enables checks and balances, fostering a culture of accountability for data and its curation, and for models and their deployment. We hope that our work provides a small step in this direction.</blockquote>
</div><!-- #content -->
<div id="footer">
<p>We gratefully acknowledge funding from the Dutch Research Council for the project <em><a href="https://markdingemanse.net/elpaco" target="_blank">Elementary Particles of Conversation</a></em> (016.vidi.185.205), and support from the CLS Humanities Lab (<a href="https://github.com/timjzee/" target="_blank">timjzee</a>)</p>
<p class="copyright">Website &amp; code © 2023 by the authors. If you find any of this useful, the paper provides the canonical and most durable citation.</p>
<p id="build-time">Table last built on 2023-07-05 at 18:04 UTC</p>
</div>
</body>
</html>
