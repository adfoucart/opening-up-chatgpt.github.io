<!DOCTYPE html>

<html>
<head>
<title>Opening up ChatGPT: LLM openness leaderboard</title>
<link href="styles.css" rel="stylesheet"/>
<link href="favicon.png" rel="icon" type="image/x-icon"/>
<script data-domain="opening-up-chatgpt.github.io" defer="" src="https://plausible.io/js/script.js"></script>
</head>
<body>
<div id="header">
<h1><a href="" title="Opening up ChatGPT: tracking openness, transparency, and accountability in instruction-tuned text generators"><img alt="Opening up AI logo" id="title-logo" src="logos/openchatgpt-logo-favicon-red-on-transparent.png"/>Opening up ChatGPT</a>: openness leaderboard for instruction-tuned LLM systems</h1>
</div>
<div id="content">
<p class="highlight" id="citation">Liesenfeld, A., Lopez, A. &amp; Dingemanse, M. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In <em>CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces</em>. July 19-21, Eindhoven. doi: <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">10.1145/3571884.3604316</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>).</p>
<p id="tagline">There is a growing amount of instruction-tuned text generators billing themselves as 'open source'. How open are they really? <span class="link-icon">🔗</span><a href="https://doi.org/10.1145/3571884.3604316" target="_blank">ACM paper</a> <span class="link-icon">🔗</span><a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a> <span class="link-icon">🔗</span><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/">repo</a></p>
<div id="included-table"><table>
<thead>
<tr class="main-header"><th>Project</th><th colspan="6">Availability</th><th colspan="6">Documentation</th><th colspan="2">Access</th></tr>
<tr class="second-header"><th>(maker, bases, URL)</th><th>Open code</th><th>LLM data</th><th>LLM weights</th><th>RL data</th><th>RL weights</th><th>License</th><th>Code</th><th>Architecture</th><th>Preprint</th><th>Paper</th><th>Modelcard</th><th>Datasheet</th><th>Package</th><th>API</th></tr>
</thead>
<tbody>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/bigscience/bloomz" target="_blank" title="">BLOOMZ</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="Repository provides a guided overview to all components">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#data" target="_blank" title="Data made available &amp; documented in detail in repo and preprint">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#models" target="_blank" title="Model made available on github">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/bigscience/xP3all" target="_blank" title="From the documentation 'xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts &amp; datasets across 46 of languages &amp; 16 NLP tasks'">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/bigscience/bloomz-optimizer-states/tree/main" target="_blank" title="Fine-tuned checkpoint available for download">~</a></td><td class="partial data-cell"><a href="https://bigscience.huggingface.co/blog/the-bigscience-rail-license" target="_blank" title="Code licensed under Apache 2.0, model under bespoke 'Responsible AI License' which imposes some limitations">~</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="Code well documented and actively maintained">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#create-xp3x" target="_blank" title="Architecture described in preprint, code available in github repo, recipe on HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2211.05100" target="_blank" title="Preprint (updated June 2023) of 65 pages + 10 page appendix">✔︎</a></td><td class="open data-cell"><a href="https://aclanthology.org/2023.acl-long.891/" target="_blank" title="Peer-reviewed paper of 9 pages + 114 page appendix describes the multitask finetuning (instruction tuning) of BLOOM (see preprint) to form BLOOMZ">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/bigscience/bloomz" target="_blank" title="Model card">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/bigscience/xP3" target="_blank" title="Dataset documented in dataset card at HuggingFace">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No packages published">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/spaces/bigscience/petals-api" target="_blank" title="Petals API via HuggingFace not always available ('not enough hardware capacity')">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/bigscience-workshop" target="_blank" title="bigscience-workshop">bigscience-workshop</a></td><td class="llmbase" colspan="3">LLM base: BLOOMZ, mT0</td><td class="rlbase" colspan="3">RL base: xP3</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/bloomz.yaml" target="_blank" title="bloomz.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://open-assistant.io/" target="_blank" title="">Open Assistant</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" title="Code includes guide for developers">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant/tree/main/data/datasets" target="_blank" title="Datasets documented in detail and recipes for cleaning up and downloading provided in code notebooks.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/OpenAssistant" target="_blank" title="Model weights in several variants downloadable through HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/OpenAssistant/oasst1" target="_blank" title="OpenAssistant Conversations is 'a human-generated, human-annotated assistant-style conversation corpus consisting of 161443 messages distributed across 66497 conversation trees, in 35 different languages, annotated with 461292 quality ratings' (preprint)">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF weights not separately released">✘</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/docs/faq#what-license-does-open-assistant-use" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/docs/intro" target="_blank" title="Separate website provides entry point to comprehensive documentation">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant/tree/main/model" target="_blank" title="Instructions to tune the pipeline on training data">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs//2304.07327" target="_blank" title="Preprint describes creation of OpenAssistant Conversations corpus for instruction tuning, but not the base LLM, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or published data audit found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/api" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://open-assistant.io/" target="_blank" title="LAION-AI">LAION-AI</a></td><td class="llmbase" colspan="3">LLM base: Pythia 12B</td><td class="rlbase" colspan="3">RL base: OpenAssistant Conversations</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Open-Assistant.yaml" target="_blank" title="Open-Assistant.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B" target="_blank" title="">Pythia-Chat-Base-7B-v0.16</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenDataHub" target="_blank" title="Training data curated and shared in separate repository">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B/tree/main" target="_blank" title="Model weights available via HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/laion/OIG" target="_blank" title="From the documentation 'This is our attempt to create a large instruction dataset of medium quality along with a smaller high quality instruciton dataset (OIG-small-chip2).'">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="RL weights not separately made available">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B#model-details" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenChatKit" target="_blank" title="Actively maintained repository">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenChatKit#reproducing-pythia-chat-base-7b" target="_blank" title="Architecture and recipe for reproducing model provided">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.01373" target="_blank" title="Preprint describes LM base (Pythia) but not instruction tuning details">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B" target="_blank" title="Model card partially available but fairly minimally specified">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/laion/OIG" target="_blank" title="OIG instruction dataset documented">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/togethercomputer" target="_blank" title="togethercomputer">togethercomputer</a></td><td class="llmbase" colspan="3">LLM base: EleutherAI pythia</td><td class="rlbase" colspan="3">RL base: OIG</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/pythia-chat-base-7B.yaml" target="_blank" title="pythia-chat-base-7B.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="">RedPajama-INCITE-Instruct-7B</a></td><td class="partial data-cell"><a href="https://github.com/togethercomputer/redpajama.cpp/tree/master/examples/redpajama" target="_blank" title="Code for datasets made available in exemplary ways; code for training and tuning harder to find">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T" target="_blank" title="RedPajama-Data-1T made available on HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base" target="_blank" title="Base is RedPajama-INCITE-7B-Base">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct" target="_blank" title="The model was trained on a large collection of diverse data, including Chain-of-Thought (CoT), Public Pool of Prompts (P3) dataset, Natural-Instructions (NI) dataset.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="Instruction-tuned version made available in paralle with base version">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct/blob/main/README.md" target="_blank" title="Models licensed under Apache 2.0, but note that the data itself is variably licensed and so imposes some limitations.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code for base LLM and instruction tuning datasets beautifully documented; code specifying training and fine-tuning sparsely documented.">~</a></td><td class="partial data-cell"><a href="https://together.ai/blog/redpajama" target="_blank" title="Architecture detailed on model card, but crucial parts appear to be forked from GPT-NeoX">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="Model card and readme provide details on datasets and">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T" target="_blank" title="Data sheet includes links to data and recipes to create from scratch">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No separate package found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="Hosted inference API available through HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://together.ai/" target="_blank" title="TogetherComputer">TogetherComputer</a></td><td class="llmbase" colspan="3">LLM base: RedPajama-INCITE-7B-Base</td><td class="rlbase" colspan="3">RL base: various (GPT-JT recipe)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/RedPajama-INCITE-Instruct-7B.yaml" target="_blank" title="RedPajama-INCITE-Instruct-7B.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/databrickslabs/dolly" target="_blank" title="">dolly</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.01373" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.databricks.com" target="_blank" title="databricks">databricks</a></td><td class="llmbase" colspan="3">LLM base: EleutherAI pythia</td><td class="rlbase" colspan="3">RL base: databricks-dolly-15k</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/dolly.yaml" target="_blank" title="dolly.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/imoneoi/openchat" target="_blank" title="">OpenChat 3.5 7B</a></td><td class="open data-cell"><a href="https://github.com/imoneoi/openchat/tree/master/ochat" target="_blank" title="Repository offers a large amount of fairly well-organized code for data curation and model">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Pretraining data for Mistral is nowhere disclosed or documented">✘</a></td><td class="open data-cell"><a href="https://github.com/mistralai/mistral-src#download-the-model" target="_blank" title="Mistral 7B weights available via Mistral repository">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Preprint says shareGPT dataset 'collected from sharegpt.com' but not disclosed or made available by this project">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/openchat/openchat_3.5/tree/main" target="_blank" title="Instruction-tuned model weights made available via HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://github.com/imoneoi/openchat/blob/master/LICENSE" target="_blank" title="Code and model released under Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/imoneoi/openchat/tree/master/ochat" target="_blank" title="There is plenty of code in the github repository but only some of it is documented">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2309.11235" target="_blank" title="Architecture quite well described in preprint">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2309.11235" target="_blank" title="Preprint describes the model architecture and instruction tuning approach, though is hampered by building on notoriously closed Llama2">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/openchat/openchat_v3.2" target="_blank" title="There is a model card that provides some details on architecture and evaluation">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="open data-cell"><a href="https://github.com/imoneoi/openchat/tree/master#installation" target="_blank" title="Python package 'ochat' provided through pip">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Model too large to load onto HuggingFace free inference API, so only available through Inference Endpoints or package">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/imoneoi" target="_blank" title="Tshinghua University">Tshinghua University</a></td><td class="llmbase" colspan="3">LLM base: Mistral 7B</td><td class="rlbase" colspan="3">RL base: ShareGPT with C-RLFT</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/OpenChat.yaml" target="_blank" title="OpenChat.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct" target="_blank" title="">MPT-30B Instruct</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/tree/main/llmfoundry/models/mpt" target="_blank" title="Codebase part of LLM foundry">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/c4" target="_blank" title="C4 is part of the dataset but a precise specification of source data is hard to find">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct/tree/main" target="_blank" title="Weights available via HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/mosaicml/dolly_hhrlhf" target="_blank" title="dolly-hhrlhf, combination of Databrick dolly-15k dataset and a filtered subset of Anthropic HH-RLHF">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="CC-by-SA 3.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/" target="_blank" title="LLM Foundry codebase is well-documented and in active development.">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct" target="_blank" title="Architecture reasonably well-documented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct" target="_blank" title="Modelcard is somewhat lacking in detail">~</a></td><td class="closed data-cell"><a href="https://www.mosaicml.com/blog/mpt-30b" target="_blank" title="Datasheet not available; data somewhat documented in blog post at link">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="API via HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.mosaicml.com" target="_blank" title="MosaicML">MosaicML</a></td><td class="llmbase" colspan="3">LLM base: MosaicML</td><td class="rlbase" colspan="3">RL base: dolly, anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/MPT-30b-instruct.yaml" target="_blank" title="MPT-30b-instruct.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="">MPT-7B Instruct</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/tree/main/llmfoundry/models/mpt" target="_blank" title="Codebase part of LLM foundry">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/c4" target="_blank" title="C4 is part of the dataset but a precise specification of source data is hard to find">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct/tree/main" target="_blank" title="Weights available via HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/mosaicml/dolly_hhrlhf" target="_blank" title="dolly-hhrlhf, combination of Databrick dolly-15k dataset and a filtered subset of Anthropic HH-RLHF">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="CC-by-SA 3.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/" target="_blank" title="LLM Foundry codebase is well-documented and in active development">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="Architecture reasonably well-documented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.mosaicml.com" target="_blank" title="MosaicML">MosaicML</a></td><td class="llmbase" colspan="3">LLM base: MosaicML</td><td class="rlbase" colspan="3">RL base: dolly, anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/MPT-7b-instruct.yaml" target="_blank" title="MPT-7b-instruct.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/CarperAI/trlx" target="_blank" title="">trlx</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/CarperAI/trlx" target="_blank" title="carperai">carperai</a></td><td class="llmbase" colspan="3">LLM base: various (pythia, flan, OPT)</td><td class="rlbase" colspan="3">RL base: various</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/trlx.yaml" target="_blank" title="trlx.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-v1.3" target="_blank" title="Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.">Vicuna 13B v 1.3</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat" target="_blank" title="Actively maintained repository">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset" target="_blank" title="Vicuna is fine-tuned LLaMA, and LLaMA in turn is based on 'publicly available datasets' that are not all specified or easily downloadable.">~</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" title="Unlike Vicuna 13B v0, these weights do not require applying delta">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/lm-sys/FastChat#fine-tuning" target="_blank" title="From the documentation 'We will not release the ShareGPT dataset'. Also 'Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 140K conversations collected from ShareGPT.com.'">✘</a></td><td class="closed data-cell"><a href="https://github.com/lm-sys/FastChat#fine-tuning" target="_blank" title="No model weights are shared for the instruction tuning">✘</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" title="From the documentation 'Vicuna is based on LLaMA and should be used under LLaMA's model license.'">~</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat" target="_blank" title="Code is quite well-documented and released as part of the FastChat framework.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2306.05685.pdf" target="_blank" title="Preprint covers training of the Vicuna model.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-v1.3" target="_blank" title="Minimal model card, but many details are not provided or have to be pieced together from elsewhere.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet provided.">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/fschat/0.1.2/" target="_blank" title="Available via pip">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat#api" target="_blank" title="Support provided for several APIs OpenAI restful, HuggingFace, Langchain">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://lmsys.org/" target="_blank" title="LMSYS">LMSYS</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: ShareGPT</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/vicuna13B-lmsys.yaml" target="_blank" title="vicuna13B-lmsys.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="hhttps://github.com/ethanyanjiali/minChatGPT" target="_blank" title="">minChatGPT</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/ethanyanjiali/minChatGPT" target="_blank" title="ethanyanjiali">ethanyanjiali</a></td><td class="llmbase" colspan="3">LLM base: GPT2</td><td class="rlbase" colspan="3">RL base: anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/minChatGPT.yaml" target="_blank" title="minChatGPT.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/BlinkDL/ChatRWKV" target="_blank" title="">ChatRWKV</a></td><td class="open data-cell"><a href="https://github.com/BlinkDL/ChatRWKV" target="_blank" title="Various community-contributed enhancements available">✔︎</a></td><td class="partial data-cell"><a href="https://pile.eleuther.ai/" target="_blank" title="Trained on The Pile. Recent versions also build on Red Pajama (https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)">~</a></td><td class="open data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-world/tree/main" target="_blank" title="Model weights released across different HuggingFace spaces">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Instruction tuning data not separately available. Documentation 'These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more'">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Weights not separately available.">✘</a></td><td class="open data-cell"><a href="https://github.com/BlinkDL/ChatRWKV/blob/main/LICENSE" target="_blank" title="Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code documentation scattered across github repo and HuggingFace spaces">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Architecture described in preprint (LM part) but not all details clearly documented.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2305.13048" target="_blank" title="Preprint covers only LLM (RNN based), not instruction fine-tuning, so partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or published data audit known">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" title="No modelcard, HuggingFace spaces only used to share files">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" title="No data sheet, HuggingFac spaces only used to share files">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/rwkv/" target="_blank" title="Available through pip install rwkv">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="API via HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.rwkv.com/" target="_blank" title="BlinkDL/RWKV">BlinkDL/RWKV</a></td><td class="llmbase" colspan="3">LLM base: RWKV-LM</td><td class="rlbase" colspan="3">RL base: alpaca, shareGPT (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ChatRWKV.yaml" target="_blank" title="ChatRWKV.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/Cerebras" target="_blank" title="">Cerebras-GPT-111M</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.03208" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/Cerebras" target="_blank" title="Cerebras + Schramm">Cerebras + Schramm</a></td><td class="llmbase" colspan="3">LLM base: </td><td class="rlbase" colspan="3">RL base: Alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Cerebras-GPT-111m.yaml" target="_blank" title="Cerebras-GPT-111m.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/LianjiaTech/BELLE" target="_blank" title="">BELLE</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Open for variants based on BLOOMZ. Closed for variants based on LLaMA, whose pretraining data is nowhere disclosed or documented.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="LLaMA based but copyright status unclear">~</a></td><td class="partial data-cell"><a href="https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M" target="_blank" title="Synthetic BELLE training data in Chinese released in batches">~</a></td><td class="partial data-cell"><a href="https://github.com/LianjiaTech/BELLE/tree/main/models" target="_blank" title="Some models available, most only as delta weights requiring separate access to LLaMA">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Lowest common denominator is non-OSI approved LLaMA licence agreement">✘</a></td><td class="partial data-cell"><a href="https://github.com/LianjiaTech/BELLE/blob/main/README_en.md" target="_blank" title="Quite some documentation on Github, though not all well-organized">~</a></td><td class="open data-cell"><a href="https://github.com/LianjiaTech/BELLE/blob/main/README_en.md" target="_blank" title="Specified in a fair bit of detail on github">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2303.14742" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card found">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="No data sheet found">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No dedicated package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API found">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="http://www.ke.com" target="_blank" title="KE Technologies">KE Technologies</a></td><td class="llmbase" colspan="3">LLM base: LLaMA &amp; BLOOMZ</td><td class="rlbase" colspan="3">RL base: alpaca, shareGPT, Belle (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/BELLE.yaml" target="_blank" title="BELLE.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank" title="Empowering Large Pre-Trained Language Models to Follow Complex Instructions">WizardLM 13B v1.2</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Fast-evolving repository contains WizardLM code">~</a></td><td class="closed data-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-2-chat.yaml" target="_blank" title="Based on LLaMA2, which is claimed to be public but nowhere exactly documented.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Based on LLaMA2 weights, which are made conditionally available by Meta.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="The Evol-Instruct V2 dataset contains 196k instruction-following sequences generated from Evol-Instruct">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank" title="Model weights offered in HuggingFace repository">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE" target="_blank" title="Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0">~</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Code is only partially documented, not clearly versioned, and appears to be in flux.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Architecture described in preprint and partly accessible in code repository">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank" title="Model card is only a placeholder and generates an error (missing yaml metadata)">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="Dataset card for Evol-Instruct generates an error">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/nlpxucan" target="_blank" title="Microsoft &amp; Peking University">Microsoft &amp; Peking University</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2-13B</td><td class="rlbase" colspan="3">RL base: Evol-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/wizardlm-13B.yaml" target="_blank" title="wizardlm-13B.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1" target="_blank" title="">Airoboros L2 70B GPT4</a></td><td class="partial data-cell"><a href="https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19" target="_blank" title="Repo exists for RL data but only a gist exists for model training and architecture">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Llama2 training data is nowhere documented or disclosed">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Llama2, made conditionally available by Meta">~</a></td><td class="open data-cell"><a href="https://github.com/jondurbin/airoboros" target="_blank" title="Airoboros, an implementation of the Self-Instruct paper">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1/tree/main" target="_blank" title="Made available through HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1#licence-and-usage-restrictions" target="_blank" title="Licensing left ambiguous because of murky status of OpenAI-derived Self-Instruct data">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="What little code available is not very systematically documented">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1/discussions/2#64c29e4c617b36543dedac9a" target="_blank" title="Some info can be gleaned at link but most remains undocumented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.4" target="_blank" title="Instructs reader to look up model card for prior 65B Llama1 version">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1" target="_blank" title="Datasheet for RL data only">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API found">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/jondurbin" target="_blank" title="Jon Durbin">Jon Durbin</a></td><td class="llmbase" colspan="3">LLM base: Llama2</td><td class="rlbase" colspan="3">RL base: Airoboros (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/airoboros.yaml" target="_blank" title="airoboros.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a 1t="" 6.2="" able="" about="" and="" answers="" are="" billion="" bootstrap,="" by="" chatglm-6b="" chatgpt,="" chinese="" corpus,="" dialogue.="" english="" feedback="" feedback.="" fine-tuning,="" for="" generate="" href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md" human="" in="" is="" learning="" line="" model="" of="" only="" optimized="" parameters,="" preference.""="" qa="" reinforcement="" similar="" supervised="" supplemented="" target="_blank" technology="" that="" the="" title="From the readme, " to="" tokens="" trained="" uses="" wit="" with="">ChatGLM-6B</a></td><td class="partial data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md#deployment" target="_blank" title="Some code made available on Github">~</a></td><td class="partial data-cell"><a href="http://doi.org/10.18653/v1/2022.acl-long.26" target="_blank" title="Training data not centrally made available, but described in 2022 ACL paper, appears to be mostly public datasets">~</a></td><td class="open data-cell"><a href="https://huggingface.co/THUDM/chatglm-6b/tree/main" target="_blank" title="Model made available through HuggingFace">✔︎</a></td><td class="closed data-cell"><a and="" are="" bootstrap,="" but="" clearly="" datasets="" feedback="" feedback",="" fine-tuning,="" href="" human="" learning="" none="" of="" reinforcement="" specified."="" supervised="" target="_blank" the="" title="docs mention " used="" wit="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No weights or checkpoints corresponding to the delta of the LLM vs RLHF provided">✘</a></td><td class="open data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/LICENSE" target="_blank" title="Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README_en.md" target="_blank" title="Some documentation available, but a lot of code is not commented or explained.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Full details architecture not specified in a single place">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="https://aclanthology.org/2022.acl-long.26/" target="_blank" title="ACL 2022 paper describes the training of the GLM base model, but the RLHF portion is more recent (there is also a related ICLR paper for a newer generation https://openreview.net/forum?id=-Aw0rrrPUF)">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/THUDM/chatglm-6b" target="_blank" title="No modelcard; the HuggingFace modelcard spot is used just as the homepage for the model.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package">✘</a></td><td class="open data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md#api-deployment" target="_blank" title="API provided through fastapi uvicorn">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/THUDM" target="_blank" title="THUDM">THUDM</a></td><td class="llmbase" colspan="3">LLM base: GLM (own)</td><td class="rlbase" colspan="3">RL base: Unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ChatGLM-6B.yaml" target="_blank" title="ChatGLM-6B.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank" title="">Mistral 7B-Instruct</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="repository provides 'minimal code to run our 7B model'">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information provided on pretraining data">✘</a></td><td class="open data-cell"><a href="https://github.com/mistralai/mistral-src#download-the-model" target="_blank" title="Base LLM model made available for download">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank" title="No information provided expect that instruction tuning is done using an unspecified 'variety of publicly available conversation datasets'">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/tree/main" target="_blank" title="Instruct version of the model made available but no information on fine-tuning procedure provided">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/README.md" target="_blank" title="Apache 2.0">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="the little code that is available is uncommented and undocumented">✘</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="Some information on architecture provided in github repo">~</a></td><td class="partial data-cell"><a href="http://arxiv.org/abs/2310.06825" target="_blank" title="Preprint rehashes marketing blurbs also given in blog and provides no details about pretraining datasets, instruction tuning datasets, or fine-tuning process, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer reviewed paper available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available, HuggingFace modelcard just points to a corporate blog post">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet available">✘</a></td><td class="partial data-cell"><a href="https://docs.mistral.ai/quickstart/" target="_blank" title="Docker image shared on github">~</a></td><td class="open data-cell"><a href="https://docs.mistral.ai/api" target="_blank" title="API specification provided by vLLM">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://mistral.ai/" target="_blank" title="Mistral AI">Mistral AI</a></td><td class="llmbase" colspan="3">LLM base: unclear</td><td class="rlbase" colspan="3">RL base: unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/mistral-7B.yaml" target="_blank" title="mistral-7B.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/nlpxucan/WizardLM" target="_blank" title="Empowering Large Pre-Trained Language Models to Follow Complex Instructions">WizardLM-7B</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Fast-evolving repository contains WizardLM code">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Based on LLaMA, which is claimed to be public but nowhere exactly documented.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on LLaMA weights, which are not openly available though a leaked versions is in wide circulation.">✘</a></td><td class="open data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM#training-data" target="_blank" title="The Evol-Instruct dataset contains 70k instruction-following sequences generated from Evol-Instruct">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0/tree/main" target="_blank" title="Model weights offered as a delta to LLaMA">~</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE" target="_blank" title="Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0">~</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Code is only partially documented, not clearly versioned, and appears to be in flux.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Architecture described in preprint and partly accessible in code repository">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0" target="_blank" title="Model card is only a placeholder and generates an error (missing yaml metadata)">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="Dataset card for Evol-Instruct generates an error">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/nlpxucan" target="_blank" title="Microsoft &amp; Peking University">Microsoft &amp; Peking University</a></td><td class="llmbase" colspan="3">LLM base: LLaMA-7B</td><td class="rlbase" colspan="3">RL base: Evol-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/wizardlm-7B-V1.yaml" target="_blank" title="wizardlm-7B-V1.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets">StableVicuna-13B</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main" target="_blank" title="Some elements of the code made available through HuggingFace">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="Based on LLaMA whose pretraining data has nowhere been disclosed or documented.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta#apply-delta-weights" target="_blank" title="Model not functional out of the box as weights require a delta computation. From the docs 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="From the documentation 'The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/discussions/7" target="_blank" title="The HuggingFace community page has an open question for release of the RL model">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="CC-BY-NC-SA-4.0. License for LLaMA is more murky, hence partial. As they say 'License for the base LLaMA model's weights is Meta's non-commercial bespoke license.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main" target="_blank" title="Code is minimally documented and deployment requires non-trivial configuration, e.g. 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Architecture is described in scattered places, but there is no clear and exhaustive overview.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2302.13971" target="_blank" title="Preprint covers only the LLaMA base model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-delta-v0" target="_blank" title="Model card provides some information but is not fully worked out as recommended in model card literature.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package found">✘</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat/tree/main#api" target="_blank" title="Addressable via FastChat / HuggingFace API">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://carper.ai" target="_blank" title="CarperAI">CarperAI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: OASST1 (human), GPT4All (human), Alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/stablevicuna.yaml" target="_blank" title="stablevicuna.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="">Falcon-40B-instruct</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" is="" open="" source"="" target="_blank" title="No source code shared, even though the term " used."="">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" target="_blank" title="From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (&gt;80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct/tree/main" target="_blank" title="Model weights available through HuggingFace library">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/project-baize/baize-chatbot" target="_blank" title="RL data inherited from Baize but provenance not well-documented. From the documentation 'Falcon-40B-Instruct was finetuned on a 150M tokens from Baize mixed with 5% of RefinedWeb data.'">~</a></td><td class="closed data-cell"><a href="https://github.com/project-baize/baize-chatbot#v1" target="_blank" title="No RL weights or checkpoints made available">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="First release came with a legally murky license that was swiftly criticised and now generates a 404. Current documentation 'Falcon-40B-Instruct is made available under the Apache 2.0 license.'">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code found, therefore no documentation found.">✘</a></td><td class="partial data-cell"><a 40b="" a="" and="" baize.""="" based="" built="" by="" causal="" decoder-only="" falcon-40b="" falcon-40b-instruct="" finetuned="" href="" is="" mixture="" model="" of="" on="" parameters="" target="_blank" tii="" title="Architecture sketched on HuggingFace as ">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2306.01116" target="_blank" title="Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper known.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no datasheet available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no package.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="There is no API, and HuggingFace inference API is disabled for this model.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://falconllm.tii.ae" target="_blank" title="Technology Innovation Institute">Technology Innovation Institute</a></td><td class="llmbase" colspan="3">LLM base: Falcon 40B</td><td class="rlbase" colspan="3">RL base: Baize (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Falcon-40B-instruct.yaml" target="_blank" title="Falcon-40B-instruct.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/openbmb/UltraRM-13b" target="_blank" title="">UltraLM</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on Llama2, which means pretraining data is nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/openbmb/UltraFeedback" target="_blank" title="UltraFeedback dataset made available along with model release">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b-v2.0" target="_blank" title="Online materials appear to be in flux and several HuggingFace links generate 404 errors, hence partial">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b#model-details" target="_blank" title="Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b#model-details" target="_blank" title="Architecture sketched in online materials.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2310.01377" target="_blank" title="Preprint describes creation of UltraFeedback dataset but also offers some detail on training and architecture of UltraRM models">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b#model-details" target="_blank" title="Model card on HuggingFace available for 13b (Llama1) model, but not for newer releases, hence partial">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/openbmb/UltraFeedback" target="_blank" title="Datasheet available for RLHF portion, but not for Llama2-based pretraining data, hence partial">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="API not provided and model too big for HuggingFace inference API">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.openbmb.org/" target="_blank" title="OpenBMB">OpenBMB</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: UltraFeedback (part synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ultraLM.yaml" target="_blank" title="ultraLM.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/" target="_blank" title="From the documentation 'Koala is fine-tuned on freely available interaction data scraped from the web, but with a specific focus on data that includes interaction with highly capable closed-source models such as ChatGPT.'">Koala 13B</a></td><td class="open data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Code scattered across projects and repositories">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/koala_data_pipeline" target="_blank" title="Repo contains data pipeline for preprocessing. Based on LLaMA which is said to be based on 'publicly available datasets' which are not made directly available.">~</a></td><td class="partial data-cell"><a href="https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl" target="_blank" title="Model weights only made available as a diff against LLaMA. OpenLLaMA provides a possible alternative?">~</a></td><td class="partial data-cell"><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/#datasets-and-training" target="_blank" title="Datasets described in blog post but not all made available">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RL weights not made available separately">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/young-geng/koala#license" target="_blank" title="Licensing is 'subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT'">~</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Code scattered across various repositories and not systematically documented.">~</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Architecture visually illustrated and described in some detail.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No systematic data sheet available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://bair.berkeley.edu/" target="_blank" title="BAIR">BAIR</a></td><td class="llmbase" colspan="3">LLM base: LLaMA 13B</td><td class="rlbase" colspan="3">RL base: HC3, ShareGPT, alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/koala.yaml" target="_blank" title="koala.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2" target="_blank" title="">Stable Beluga 2</a></td><td class="closed data-cell"><a href="" target="_blank" title="No repository with open code related to training, fine-tuning or evaluation found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Pretraining data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Says: 'Stable Beluga 2 is trained on our internal Orca-style dataset', 'created synthetically using high-quality instructions' from COT Submix Original, NIV2 Submix Original, FLAN 2021 Submix Original, T0 Submix Original.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2/tree/main" target="_blank" title="Instruction-tuned model weights available">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt" target="_blank" title="Usage requires signing StabilityAI's bespoke 'Stable Beluga Non-Commercial Community License', not an OSI recognised open license">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code on HuggingFace only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2#model-details" target="_blank" title="Says 'Stable Beluga 2 is a Llama2 70B model finetuned on an Orca style Dataset'.">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/papers/2306.02707" target="_blank" title="Preprint from Microsoft describes Orca method of finetuning using GPT4-derived synthetic data, but no details of this particular architecture">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2" target="_blank" title="There is a model card, but it provides only a minimum of detail">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="model too large to run on HuggingFace free inference API">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://stability.ai/" target="_blank" title="Stability AI">Stability AI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: Orca-style (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/StableBeluga2.yaml" target="_blank" title="StableBeluga2.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" title="project_notes">Stanford Alpaca</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on LLaMA, whose pretraining data is nowhere disclosed or documented.">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="LLaMA based, copyright status unclear">~</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model.">~</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="LLaMA based">~</a></td><td class="closed data-cell"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform" target="_blank" title="Pegged to LLaMA licence agreement">✘</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" title="Insofar as code is made available it is fairly well documented">~</a></td><td class="open data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#fine-tuning" target="_blank" title="Fair bit of documentation available on github repository">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found; uses the release-by-blogpost playbook">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed work found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No data sheet found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://crfm.stanford.edu/" target="_blank" title="Stanford University CRFM">Stanford University CRFM</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: Self-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/alpaca.yaml" target="_blank" title="alpaca.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="Falcon-180B-Chat is a 180B parameters causal decoder-only model built by TII based on Falcon-180B and finetuned on a mixture of Ultrachat, Platypus and Airoboros.">Falcon-180B-chat</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="No source code shared anywhere. The over 200 github repositories of TII appear to include no LLM or Falcon related code.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" target="_blank" title="From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (&gt;80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.">~</a></td><td class="partial data-cell"><a acceptable="" href="https://huggingface.co/tiiuae/falcon-180B" policy""="" target="_blank" title="requires signing up and accepting " use="">~</a></td><td class="partial data-cell"><a a="" airoboros"."="" and="" finetuned="" href="https://github.com/project-baize/baize-chatbot" mixture="" of="" on="" platypus="" target="_blank" title="No details provided beyond " ultrachat,="">~</a></td><td class="partial data-cell"><a acceptable="" href="https://huggingface.co/tiiuae/falcon-180B-chat/tree/main" policy"."="" target="_blank" title="No RL weights or checkpoints made available; fine-tuned 'chat' model only available after signing " use="">~</a></td><td class="closed data-cell"><a acceptable="" href="https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt" policy"."="" target="_blank" title="Released under Falcon 180B TII license (not OSI approved) and a separate " use="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code available, so no code documented.">✘</a></td><td class="partial data-cell"><a "optimized="" and="" available."="" but="" causal="" decoder-only"="" details="" few="" for="" href="https://huggingface.co/tiiuae/falcon-180B-chat#model-architecture-and-objective" inference",="" target="_blank" title="Architecture described as  " very="">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2306.01116" target="_blank" title="Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper known.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no datasheet available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no package.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="There is no API, and HuggingFace inference API is disabled for this model.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://falconllm.tii.ae" target="_blank" title="Technology Innovation Institute">Technology Innovation Institute</a></td><td class="llmbase" colspan="3">LLM base: Falcon 180B</td><td class="rlbase" colspan="3">RL base: OpenPlatypus, Ultrachat, Airoboros (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Falcon-180B-chat.yaml" target="_blank" title="Falcon-180B-chat.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama/" target="_blank" title="">LLaMA2 Chat</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/tree/main" target="_blank" title="Repository only offers 'a minimal example to load Llama 2 models and run inference'; no training, fine-tuning, evaluation code made available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF data including 1 million Meta-specific tuning prompts not made available (even as it incorporates some open RLHF datasets)">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" target="_blank" title="Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" target="_blank" title="Architecture sketched in preprint, though many details missing.">~</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" target="_blank" title="Corporate preprint quite some detail on pretraining, RLHF, and safety measures but none on datasets.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md" target="_blank" title="There is a model card, but it provides the absolute minimum of detail, and none whatsoever on training data.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API only available behind a privacy-defying signup form">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/facebookresearch" target="_blank" title="Facebook Research">Facebook Research</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: Meta, StackExchange, Anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-2-chat.yaml" target="_blank" title="llama-2-chat.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" title="HuggingFace profile says 'Solar is a great example of the progress enabled by open source.'">Solar 70B</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code repository found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No RLHF datasets specified or shared, docs say 'Orca-style dataset, Alpaca-style dataset'">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit/tree/main" target="_blank" title="Finetuned checkpoints only shared through CC-BY-NC">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit#model-details" target="_blank" title="Meta Community License for base model, and CC-BY-NC 4.0 for fine-tuned model weights">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="HuggingFace code only comprises configuration json; no documentation available.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" title="Precise architecture, training, fine-tuning procedures not given.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint or any form of scientific docuentation found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" title="HuggingFace model card used mostly as advertising, omits many details on training, fine-tuning, evaluation.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API only available by signing up for 'private LLM' service">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://en.upstage.ai/" target="_blank" title="Upstage AI">Upstage AI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: Orca-style, Alpaca-style</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/solar-70B.yaml" target="_blank" title="solar-70B.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://chat.openai.com/" target="_blank" title="NA">ChatGPT</a></td><td class="closed data-cell"><a href="https://chat.openai.com/" target="_blank" title="OpenAI has not released any source code related to ChatGPT">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released or documented any of the source training data">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released model weights for GPT3.5">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released the instruction-tuning data">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released details about RLHF models weights">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="None of the code is open sourced; license unknown.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code is not available, documentation level unknown.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not clearly documented the LLM+RLHF architecture or its evaluation.">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2203.02155" target="_blank" title="Preprint describes only the instruction-tuning method; no further papers available.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper documenting this system is available.">✘</a></td><td class="partial data-cell"><a href="https://github.com/openai/gpt-3/blob/master/model-card.md" target="_blank" title="No modelcard is available for GPT3.5. The linked model card is for GPT3 is dated Sept 2020.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released a datasheet or any other documentation or evaluation of source data.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Some third party packages exist; support for them is contingent on OpenAI's whims.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="API access is only available for commercial users.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://openai.com/" target="_blank" title="OpenAI">OpenAI</a></td><td class="llmbase" colspan="3">LLM base: GPT 3.5</td><td class="rlbase" colspan="3">RL base: Instruct-GPT</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/chatgpt.yaml" target="_blank" title="chatgpt.yaml">§</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1" target="_blank" title="Xwin-LM aims to develop and open-source alignment technologies for large language models">Xwin-LM</a></td><td class="closed data-cell"><a :"="" code'"="" href="https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1" release="" source="" target="_blank" the="" title="HuggingFace page notes 'to do ">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF data for Llama includes 1 million Meta-specific tuning prompts not made available, no other details known about RLHF and alignment tuning added by Xwin-LM">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1/tree/main" target="_blank" title="Downloadable model presumably includes RLHF tuning but no documentation available">✘</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" target="_blank" title="Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No documentation available.">✘</a></td><td class="closed data-cell"><a crucial="" href="https://github.com/Xwin-LM/Xwin-LM#news" plays="" rlhf="" role""="" target="_blank" title="No information available beyond that it is based on Llama and ">✘</a></td><td class="closed data-cell"><a (stay="" coming="" href="https://github.com/Xwin-LM/Xwin-LM#news" soon="" target="_blank" title="No preprint available; " tuned)""="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper available">✘</a></td><td class="closed data-cell"><a advertise="" available."="" but="" card"="" details="" href="https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1" model="" model,="" no="" target="_blank" title="HuggingFace " to="" used="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API available through vllm and HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/Xwin-LM" target="_blank" title="Xwin-LM">Xwin-LM</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: unknown</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Xwin-LM.yaml" target="_blank" title="Xwin-LM.yaml">§</a></td></tr>
</tbody>
</table>
</div>
<p id="table-guide"><em>How to use this table.</em> Every cell records a three-level openness judgement (<span class="openness open"><strong>✔︎</strong> open</span>, <span class="openness partial"><strong>~</strong> partial</span> or <span class="openness closed"><strong>✘</strong> closed</span>) with a direct link to the available evidence; on hover, the cell will display the notes we have on file for that judgement. At the end of a row, the <strong>§</strong> is a direct link to source data. The table is sorted by cumulative openness, where <strong>✔︎</strong> is 1, <strong>~</strong> is 0.5 and <strong>✘</strong> is 0 points. Note that RL may refer to RLHF or other forms of fine-tuning aimed at fostering instruction-following behaviour.</p>
<h2>Why is openness important?</h2>
<p>Open research is the lifeblood of cumulative progress in science and engineering. Openness is key for fundamental research, for fostering critical computational literacy, and for making informed choices for or against deployment of instruction-tuned LLM architectures. The closed &amp; proprietary nature of ChatGPT and kin makes them fundamentally unfit for responsible use in research and education.</p>
<p>Open alternatives provide ways to build reproducible workflows, chart resource costs, and lessen reliance on corporate whims. One aim of our work here is to provide tools to track openness, transparency and accountability in the fast-evolving landscape of instruction-tuned text generators. Read more in the <a href="https://dl.acm.org/doi/10.1145/3571884.3604316" target="_blank">paper</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>) or <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/" target="_blank">contribute to the repo</a>.</p>
<p class="highlight" id="contribute">If you know a model that should be listed here or a data point that needs updating, please see <a href="https://github.com/opening-up-chatgpt/">guidelines for contributors</a>. We welcome any contribution, whether it's a quick addition to our <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt">awesomelist</a> or a more detail-oriented contribution to the metadata for a <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/tree/main/projects">specific project</a>.</p>
<h2>TL;DR</h2>
<p>Our paper makes the following contributions:</p>
<ul>
<li>We review the risks of relying on proprietary software</li>
<li>We review best practices for open, transparent and accountable 'AI'</li>
<li>We find over 20 ChatGPT alternatives at varying degrees of openness, development and documentation</li>
<li>We argue that tech is never a <em>fait accompli</em> unless we make it so, and that openness enables critical computational literacy</li>
</ul>
<p>We find the following recurrent patterns:</p>
<ul>
<li>Many projects inherit data of dubious legality</li>
<li>Few projects share the all-important instruction-tuning</li>
<li>Preprints are rare, peer-reviewed papers even rarer</li>
<li>Synthetic instruction-tuning data is on the rise, with unknown consequences that are in need of research</li>
</ul>
<p>We conclude as follows:</p>
<blockquote id="conclusion">Openness is not the full solution to the scientific and ethical challenges of conversational text generators. Open data will not mitigate the harmful consequences of thoughtless deployment of large language models, nor the questionable copyright implications of scraping all publicly available data from the internet. However, openness does make original research possible, including efforts to build reproducible workflows and understand the fundamentals of instruction-tuned LLM architectures. Openness also enables checks and balances, fostering a culture of accountability for data and its curation, and for models and their deployment. We hope that our work provides a small step in this direction.
  </blockquote>
<p>Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In <em>CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces</em>. July 19-21, Eindhoven. doi: <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">10.1145/3571884.3604316</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>).</p>
</div><!-- #content -->
<div id="footer">
<p>We gratefully acknowledge funding from the Dutch Research Council for the project <em><a href="https://markdingemanse.net/elpaco" target="_blank">Elementary Particles of Conversation</a></em> (016.vidi.185.205), and support from the CLS Humanities Lab (<a href="https://github.com/timjzee/" target="_blank">timjzee</a>)</p>
<p class="copyright">Website &amp; code © 2023 by the authors. If you find any of this useful, the paper provides the canonical and most durable citation.</p>
<p id="build-time">Table last built on 2023-11-04 at 20:05 UTC</p>
</div>
</body>
</html>
