---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here: 
# https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/tree/main/projects#criteria

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the stipulation 
# stipulation that attribution should come in the form of a link to http://opening-up-chatgpt.github.io
# and a citation to the paper in which the initial dataset & criteria were published:

# Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces. July 19-21, Eindhoven. doi: 10.1145/3571884.3604316 

project:
    name: RedPajama-INCITE-Instruct-7B
    link: https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct
    notes:
    llmbase: RedPajama-INCITE-7B-Base
    rlbase: various (GPT-JT recipe)
    license: Apache 2.0

org:
    name: TogetherComputer
    link: https://together.ai/
    notes:

# availability:
opencode:
    class: partial
    link: https://github.com/togethercomputer/redpajama.cpp/tree/master/examples/redpajama
    notes: Code for datasets made available in exemplary ways; code for training and tuning harder to find

llmdata:
    class: open
    link: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T
    notes: RedPajama-Data-1T made available on HuggingFace

llmweights:
    class: open
    link: https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base
    notes: Base is RedPajama-INCITE-7B-Base

rldata:
    class: open
    link: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct
    notes: The model was trained on a large collection of diverse data, including Chain-of-Thought (CoT), Public Pool of Prompts (P3) dataset, Natural-Instructions (NI) dataset.

rlweights:
    class: open
    link: https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct
    notes: Instruction-tuned version made available in paralle with base version

license:
    class: partial
    link: https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct/blob/main/README.md
    notes: Models licensed under Apache 2.0, but note that the data itself is variably licensed and so imposes some limitations.

# documentation:
code:
    class: partial
    link:
    notes: Code for base LLM and instruction tuning datasets beautifully documented; code specifying training and fine-tuning sparsely documented.

architecture:
    class: partial
    link: https://together.ai/blog/redpajama
    notes: Architecture detailed on model card, but crucial parts appear to be forked from GPT-NeoX 

preprint:
    class: closed
    link:
    notes: No preprint found

paper:
    class: closed
    link:
    notes: No paper found

modelcard:
    class: open
    link: https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct
    notes: Model card and readme provide details on datasets and 

datasheet:
    class: open
    link: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T
    notes: Data sheet includes links to data and recipes to create from scratch

# access:
package:
    class: closed
    link:
    notes: No separate package found

api:
    class: partial
    link: https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct
    notes: Hosted inference API available through HuggingFace
